{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# Suppress TensorFlow logging messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from basenji import seqnn, stream, dataset, dna_io\n",
    "\n",
    "from akita_utils.numpy_utils import ut_dense\n",
    "from akita_utils.dna_utils import dna_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data parameters\n",
    "mouse_dir = \"/project/fudenber_735/tensorflow_models/akita/v2/data/mm10/\"\n",
    "data_stats_file = '%s/statistics.json' % mouse_dir\n",
    "\n",
    "# human_dir = \"/project/fudenber_735/tensorflow_models/akita/v2/data/hg38/\"\n",
    "# data_stats_file = '%s/statistics.json' % human_dir\n",
    "\n",
    "with open(data_stats_file) as data_stats_open:\n",
    "    data_stats = json.load(data_stats_open)\n",
    "seq_length = data_stats['seq_length']\n",
    "target_length = data_stats['target_length']\n",
    "hic_diags =  data_stats['diagonal_offset']\n",
    "target_crop = data_stats['crop_bp'] // data_stats['pool_width']\n",
    "target_length1 = data_stats['seq_length'] // data_stats['pool_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model's parameters\n",
    "batch_size=8 \n",
    "head_index = 1 # mouse!\n",
    "# head_index = 0 # human!\n",
    "shifts = \"0\"\n",
    "rc = False\n",
    "shifts = [int(shift) for shift in shifts.split(\",\")]\n",
    "\n",
    "# directory with models\n",
    "models_dir = \"/project/fudenber_735/tensorflow_models/akita/v2/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open genome FASTA\n",
    "genome_fasta = \"/project/fudenber_735/genomes/mm10/mm10.fa\"\n",
    "genome_open = pysam.Fastafile(genome_fasta)\n",
    "\n",
    "# picking the model\n",
    "model_index = 0\n",
    "# model_index = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " sequence (InputLayer)       [(None, 1310720, 4)]         0         []                            \n",
      "                                                                                                  \n",
      " stochastic_reverse_complem  ((None, 1310720, 4),         0         ['sequence[0][0]']            \n",
      " ent (StochasticReverseComp   ())                                                                 \n",
      " lement)                                                                                          \n",
      "                                                                                                  \n",
      " stochastic_shift (Stochast  (None, 1310720, 4)           0         ['stochastic_reverse_complemen\n",
      " icShift)                                                           t[0][0]']                     \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 1310720, 4)           0         ['stochastic_shift[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 1310720, 128)         7680      ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 1310720, 128)         512       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 655360, 128)          0         ['batch_normalization[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 655360, 128)          0         ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 655360, 128)          81920     ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 655360, 128)          512       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 327680, 128)          0         ['batch_normalization_1[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 327680, 128)          0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 327680, 128)          81920     ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 327680, 128)          512       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPoolin  (None, 163840, 128)          0         ['batch_normalization_2[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 163840, 128)          0         ['max_pooling1d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 163840, 128)          81920     ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 163840, 128)          512       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPoolin  (None, 81920, 128)           0         ['batch_normalization_3[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 81920, 128)           0         ['max_pooling1d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 81920, 128)           81920     ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 81920, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 40960, 128)           0         ['batch_normalization_4[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 40960, 128)           0         ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 40960, 128)           81920     ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 40960, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 20480, 128)           0         ['batch_normalization_5[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 20480, 128)           0         ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 20480, 128)           81920     ['re_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 20480, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 10240, 128)           0         ['batch_normalization_6[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 10240, 128)           0         ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 10240, 128)           81920     ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 10240, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 5120, 128)            0         ['batch_normalization_7[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 5120, 128)            0         ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 5120, 128)            81920     ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 5120, 128)            512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPoolin  (None, 2560, 128)            0         ['batch_normalization_8[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 2560, 128)            0         ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 2560, 128)            81920     ['re_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 2560, 128)            512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, 1280, 128)            0         ['batch_normalization_9[0][0]'\n",
      " g1D)                                                               ]                             \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 1280, 128)            0         ['max_pooling1d_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1280, 128)            81920     ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 1280, 128)            512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooli  (None, 640, 128)             0         ['batch_normalization_10[0][0]\n",
      " ng1D)                                                              ']                            \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 640, 128)             0         ['max_pooling1d_10[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 640, 64)              24576     ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 640, 64)              256       ['conv1d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 640, 64)              0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 640, 128)             8192      ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 640, 128)             512       ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 640, 128)             0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 640, 128)             0         ['max_pooling1d_10[0][0]',    \n",
      "                                                                     'dropout[0][0]']             \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 640, 128)             0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 640, 64)              24576     ['re_lu_13[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 640, 64)              256       ['conv1d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 640, 64)              0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 640, 128)             8192      ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 640, 128)             512       ['conv1d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 640, 128)             0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 640, 128)             0         ['add[0][0]',                 \n",
      "                                                                     'dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 640, 128)             0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 640, 64)              24576     ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 640, 64)              256       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 640, 64)              0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 640, 128)             8192      ['re_lu_16[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 640, 128)             512       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 640, 128)             0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 640, 128)             0         ['add_1[0][0]',               \n",
      "                                                                     'dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 640, 128)             0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 640, 64)              24576     ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 640, 64)              256       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 640, 64)              0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 640, 128)             8192      ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 640, 128)             512       ['conv1d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 640, 128)             0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 640, 128)             0         ['add_2[0][0]',               \n",
      "                                                                     'dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 640, 128)             0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 640, 64)              24576     ['re_lu_19[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 640, 64)              256       ['conv1d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)             (None, 640, 64)              0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)          (None, 640, 128)             8192      ['re_lu_20[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 640, 128)             512       ['conv1d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 640, 128)             0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 640, 128)             0         ['add_3[0][0]',               \n",
      "                                                                     'dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)             (None, 640, 128)             0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)          (None, 640, 64)              24576     ['re_lu_21[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 640, 64)              256       ['conv1d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)             (None, 640, 64)              0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)          (None, 640, 128)             8192      ['re_lu_22[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 640, 128)             512       ['conv1d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 640, 128)             0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 640, 128)             0         ['add_4[0][0]',               \n",
      "                                                                     'dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)             (None, 640, 128)             0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)          (None, 640, 64)              24576     ['re_lu_23[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 640, 64)              256       ['conv1d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)             (None, 640, 64)              0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)          (None, 640, 128)             8192      ['re_lu_24[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 640, 128)             512       ['conv1d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 640, 128)             0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 640, 128)             0         ['add_5[0][0]',               \n",
      "                                                                     'dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)             (None, 640, 128)             0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)          (None, 640, 64)              24576     ['re_lu_25[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 640, 64)              256       ['conv1d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)             (None, 640, 64)              0         ['batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)          (None, 640, 128)             8192      ['re_lu_26[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 640, 128)             512       ['conv1d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 640, 128)             0         ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 640, 128)             0         ['add_6[0][0]',               \n",
      "                                                                     'dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)             (None, 640, 128)             0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)          (None, 640, 64)              24576     ['re_lu_27[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 640, 64)              256       ['conv1d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)             (None, 640, 64)              0         ['batch_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)          (None, 640, 128)             8192      ['re_lu_28[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 640, 128)             512       ['conv1d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 640, 128)             0         ['batch_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 640, 128)             0         ['add_7[0][0]',               \n",
      "                                                                     'dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)             (None, 640, 128)             0         ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)          (None, 640, 64)              24576     ['re_lu_29[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 640, 64)              256       ['conv1d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)             (None, 640, 64)              0         ['batch_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)          (None, 640, 128)             8192      ['re_lu_30[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 640, 128)             512       ['conv1d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 640, 128)             0         ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 640, 128)             0         ['add_8[0][0]',               \n",
      "                                                                     'dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)             (None, 640, 128)             0         ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)          (None, 640, 64)              24576     ['re_lu_31[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 640, 64)              256       ['conv1d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)             (None, 640, 64)              0         ['batch_normalization_31[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)          (None, 640, 128)             8192      ['re_lu_32[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 640, 128)             512       ['conv1d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 640, 128)             0         ['batch_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 640, 128)             0         ['add_9[0][0]',               \n",
      "                                                                     'dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)             (None, 640, 128)             0         ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)          (None, 640, 80)              51200     ['re_lu_33[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 640, 80)              320       ['conv1d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)             (None, 640, 80)              0         ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " one_to_two (OneToTwo)       (None, 640, 640, 80)         0         ['re_lu_34[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)             (None, 640, 640, 80)         0         ['one_to_two[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 640, 640, 80)         57600     ['re_lu_35[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 640, 640, 80)         320       ['conv2d[0][0]']              \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " symmetrize2d (Symmetrize2D  (None, 640, 640, 80)         0         ['batch_normalization_34[0][0]\n",
      " )                                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)             (None, 640, 640, 80)         0         ['symmetrize2d[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 640, 640, 40)         28800     ['re_lu_36[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 640, 640, 40)         160       ['conv2d_1[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)             (None, 640, 640, 40)         0         ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 640, 640, 80)         3200      ['re_lu_37[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 640, 640, 80)         320       ['conv2d_2[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 640, 640, 80)         0         ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 640, 640, 80)         0         ['symmetrize2d[0][0]',        \n",
      "                                                                     'dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " symmetrize2d_1 (Symmetrize  (None, 640, 640, 80)         0         ['add_11[0][0]']              \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)             (None, 640, 640, 80)         0         ['symmetrize2d_1[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 640, 640, 40)         28800     ['re_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 640, 640, 40)         160       ['conv2d_3[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)             (None, 640, 640, 40)         0         ['batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 640, 640, 80)         3200      ['re_lu_39[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 640, 640, 80)         320       ['conv2d_4[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 640, 640, 80)         0         ['batch_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 640, 640, 80)         0         ['symmetrize2d_1[0][0]',      \n",
      "                                                                     'dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " symmetrize2d_2 (Symmetrize  (None, 640, 640, 80)         0         ['add_12[0][0]']              \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)             (None, 640, 640, 80)         0         ['symmetrize2d_2[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 640, 640, 40)         28800     ['re_lu_40[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 640, 640, 40)         160       ['conv2d_5[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)             (None, 640, 640, 40)         0         ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 640, 640, 80)         3200      ['re_lu_41[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 640, 640, 80)         320       ['conv2d_6[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 640, 640, 80)         0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 640, 640, 80)         0         ['symmetrize2d_2[0][0]',      \n",
      "                                                                     'dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " symmetrize2d_3 (Symmetrize  (None, 640, 640, 80)         0         ['add_13[0][0]']              \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)             (None, 640, 640, 80)         0         ['symmetrize2d_3[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 640, 640, 40)         28800     ['re_lu_42[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 640, 640, 40)         160       ['conv2d_7[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)             (None, 640, 640, 40)         0         ['batch_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 640, 640, 80)         3200      ['re_lu_43[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 640, 640, 80)         320       ['conv2d_8[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)        (None, 640, 640, 80)         0         ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 640, 640, 80)         0         ['symmetrize2d_3[0][0]',      \n",
      "                                                                     'dropout_14[0][0]']          \n",
      "                                                                                                  \n",
      " symmetrize2d_4 (Symmetrize  (None, 640, 640, 80)         0         ['add_14[0][0]']              \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " re_lu_44 (ReLU)             (None, 640, 640, 80)         0         ['symmetrize2d_4[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 640, 640, 40)         28800     ['re_lu_44[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 640, 640, 40)         160       ['conv2d_9[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_45 (ReLU)             (None, 640, 640, 40)         0         ['batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 640, 640, 80)         3200      ['re_lu_45[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 640, 640, 80)         320       ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 640, 640, 80)         0         ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 640, 640, 80)         0         ['symmetrize2d_4[0][0]',      \n",
      "                                                                     'dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " symmetrize2d_5 (Symmetrize  (None, 640, 640, 80)         0         ['add_15[0][0]']              \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " re_lu_46 (ReLU)             (None, 640, 640, 80)         0         ['symmetrize2d_5[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 640, 640, 40)         28800     ['re_lu_46[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 640, 640, 40)         160       ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_47 (ReLU)             (None, 640, 640, 40)         0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 640, 640, 80)         3200      ['re_lu_47[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 640, 640, 80)         320       ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 640, 640, 80)         0         ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_16 (Add)                (None, 640, 640, 80)         0         ['symmetrize2d_5[0][0]',      \n",
      "                                                                     'dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " symmetrize2d_6 (Symmetrize  (None, 640, 640, 80)         0         ['add_16[0][0]']              \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " squeeze_excite (SqueezeExc  (None, 640, 640, 80)         2010      ['symmetrize2d_6[0][0]']      \n",
      " ite)                                                                                             \n",
      "                                                                                                  \n",
      " cropping2d (Cropping2D)     (None, 512, 512, 80)         0         ['squeeze_excite[0][0]']      \n",
      "                                                                                                  \n",
      " upper_tri (UpperTri)        (None, 130305, 80)           0         ['cropping2d[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_48 (ReLU)             (None, 130305, 80)           0         ['upper_tri[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 130305, 5)            405       ['re_lu_48[0][0]']            \n",
      "                                                                                                  \n",
      " switch_reverse_triu (Switc  (None, 130305, 5)            0         ['dense[0][0]',               \n",
      " hReverseTriu)                                                       'stochastic_reverse_complemen\n",
      "                                                                    t[0][1]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1508143 (5.75 MB)\n",
      "Trainable params: 1499183 (5.72 MB)\n",
      "Non-trainable params: 8960 (35.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [2048, 2048]\n",
      "target_lengths [130305, 130305]\n",
      "target_crops [-64833, -64833]\n"
     ]
    }
   ],
   "source": [
    "params_file = models_dir + f\"/f{model_index}c0\" + \"/train\" + \"/params.json\"\n",
    "model_file = models_dir + f\"/f{model_index}c0\" + \"/train\" + f\"/model{head_index}_best.h5\"\n",
    "\n",
    "# read model parameters\n",
    "with open(params_file) as params_open:\n",
    "    params = json.load(params_open)\n",
    "params_train = params[\"train\"]\n",
    "params_model = params[\"model\"]\n",
    "\n",
    "if batch_size is None:\n",
    "    batch_size = params_train[\"batch_size\"]\n",
    "else:\n",
    "    batch_size = batch_size\n",
    "\n",
    "# load model\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n",
    "seqnn_model.restore(model_file, head_i=head_index)\n",
    "seqnn_model.build_ensemble(rc, shifts)\n",
    "seq_length = int(params_model[\"seq_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_seqs_gen(\n",
    "    seq_coords_df,\n",
    "    genome_open\n",
    "):\n",
    "    for index, row in seq_coords_df.iterrows():\n",
    "\n",
    "        wt_seq_1hot = dna_1hot(\n",
    "            genome_open.fetch(row.chrom\t, row.window_start, row.window_end).upper()\n",
    "        )\n",
    "\n",
    "        yield wt_seq_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_path = \"./all_explained_boundaries.tsv\"\n",
    "unexplained_path = \"./all_unexplained_boundaries.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_tads = pd.read_csv(explained_path, sep=\"\\t\", index_col=None)\n",
    "unexplained_tads = pd.read_csv(unexplained_path, sep=\"\\t\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_tads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_stream = stream.PredStreamGen(\n",
    "        seqnn_model,\n",
    "        simple_seqs_gen(unexplained_tads, genome_open),\n",
    "        batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preds = len(unexplained_tads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from akita_utils.stats_utils import slide_diagonal_insulation, _extract_centered_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFF-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_width=29 since TAD boundary is 5 bins + 12 bins at each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 2 * 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_insulation(target_map, window=16, crop_around_center=True, crop_width=2):\n",
    "    map_size = target_map.shape[0]\n",
    "    insulation_scores = slide_diagonal_insulation(target_map, window)\n",
    "    \n",
    "    if crop_around_center:\n",
    "        insulation_scores = _extract_centered_window(\n",
    "            insulation_scores, window=window, width=3\n",
    "        )\n",
    "    \n",
    "    min_score = np.nanmin(insulation_scores)\n",
    "    \n",
    "    return min_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_index=  0\n",
      "4/4 [==============================] - 14s 494ms/step\n",
      "pred_index=  1\n",
      "pred_index=  2\n",
      "pred_index=  3\n",
      "pred_index=  4\n",
      "pred_index=  5\n",
      "pred_index=  6\n",
      "pred_index=  7\n",
      "pred_index=  8\n",
      "pred_index=  9\n",
      "pred_index=  10\n",
      "pred_index=  11\n",
      "pred_index=  12\n",
      "pred_index=  13\n",
      "pred_index=  14\n",
      "pred_index=  15\n",
      "pred_index=  16\n",
      "pred_index=  17\n",
      "pred_index=  18\n",
      "pred_index=  19\n",
      "pred_index=  20\n",
      "pred_index=  21\n",
      "pred_index=  22\n",
      "pred_index=  23\n",
      "pred_index=  24\n",
      "pred_index=  25\n",
      "pred_index=  26\n",
      "pred_index=  27\n",
      "pred_index=  28\n",
      "pred_index=  29\n",
      "pred_index=  30\n",
      "pred_index=  31\n",
      "pred_index=  32\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  33\n",
      "pred_index=  34\n",
      "pred_index=  35\n",
      "pred_index=  36\n",
      "pred_index=  37\n",
      "pred_index=  38\n",
      "pred_index=  39\n",
      "pred_index=  40\n",
      "pred_index=  41\n",
      "pred_index=  42\n",
      "pred_index=  43\n",
      "pred_index=  44\n",
      "pred_index=  45\n",
      "pred_index=  46\n",
      "pred_index=  47\n",
      "pred_index=  48\n",
      "pred_index=  49\n",
      "pred_index=  50\n",
      "pred_index=  51\n",
      "pred_index=  52\n",
      "pred_index=  53\n",
      "pred_index=  54\n",
      "pred_index=  55\n",
      "pred_index=  56\n",
      "pred_index=  57\n",
      "pred_index=  58\n",
      "pred_index=  59\n",
      "pred_index=  60\n",
      "pred_index=  61\n",
      "pred_index=  62\n",
      "pred_index=  63\n",
      "pred_index=  64\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  65\n",
      "pred_index=  66\n",
      "pred_index=  67\n",
      "pred_index=  68\n",
      "pred_index=  69\n",
      "pred_index=  70\n",
      "pred_index=  71\n",
      "pred_index=  72\n",
      "pred_index=  73\n",
      "pred_index=  74\n",
      "pred_index=  75\n",
      "pred_index=  76\n",
      "pred_index=  77\n",
      "pred_index=  78\n",
      "pred_index=  79\n",
      "pred_index=  80\n",
      "pred_index=  81\n",
      "pred_index=  82\n",
      "pred_index=  83\n",
      "pred_index=  84\n",
      "pred_index=  85\n",
      "pred_index=  86\n",
      "pred_index=  87\n",
      "pred_index=  88\n",
      "pred_index=  89\n",
      "pred_index=  90\n",
      "pred_index=  91\n",
      "pred_index=  92\n",
      "pred_index=  93\n",
      "pred_index=  94\n",
      "pred_index=  95\n",
      "pred_index=  96\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  97\n",
      "pred_index=  98\n",
      "pred_index=  99\n",
      "pred_index=  100\n",
      "pred_index=  101\n",
      "pred_index=  102\n",
      "pred_index=  103\n",
      "pred_index=  104\n",
      "pred_index=  105\n",
      "pred_index=  106\n",
      "pred_index=  107\n",
      "pred_index=  108\n",
      "pred_index=  109\n",
      "pred_index=  110\n",
      "pred_index=  111\n",
      "pred_index=  112\n",
      "pred_index=  113\n",
      "pred_index=  114\n",
      "pred_index=  115\n",
      "pred_index=  116\n",
      "pred_index=  117\n",
      "pred_index=  118\n",
      "pred_index=  119\n",
      "pred_index=  120\n",
      "pred_index=  121\n",
      "pred_index=  122\n",
      "pred_index=  123\n",
      "pred_index=  124\n",
      "pred_index=  125\n",
      "pred_index=  126\n",
      "pred_index=  127\n",
      "pred_index=  128\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  129\n",
      "pred_index=  130\n",
      "pred_index=  131\n",
      "pred_index=  132\n",
      "pred_index=  133\n",
      "pred_index=  134\n",
      "pred_index=  135\n",
      "pred_index=  136\n",
      "pred_index=  137\n",
      "pred_index=  138\n",
      "pred_index=  139\n",
      "pred_index=  140\n",
      "pred_index=  141\n",
      "pred_index=  142\n",
      "pred_index=  143\n",
      "pred_index=  144\n",
      "pred_index=  145\n",
      "pred_index=  146\n",
      "pred_index=  147\n",
      "pred_index=  148\n",
      "pred_index=  149\n",
      "pred_index=  150\n",
      "pred_index=  151\n",
      "pred_index=  152\n",
      "pred_index=  153\n",
      "pred_index=  154\n",
      "pred_index=  155\n",
      "pred_index=  156\n",
      "pred_index=  157\n",
      "pred_index=  158\n",
      "pred_index=  159\n",
      "pred_index=  160\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  161\n",
      "pred_index=  162\n",
      "pred_index=  163\n",
      "pred_index=  164\n",
      "pred_index=  165\n",
      "pred_index=  166\n",
      "pred_index=  167\n",
      "pred_index=  168\n",
      "pred_index=  169\n",
      "pred_index=  170\n",
      "pred_index=  171\n",
      "pred_index=  172\n",
      "pred_index=  173\n",
      "pred_index=  174\n",
      "pred_index=  175\n",
      "pred_index=  176\n",
      "pred_index=  177\n",
      "pred_index=  178\n",
      "pred_index=  179\n",
      "pred_index=  180\n",
      "pred_index=  181\n",
      "pred_index=  182\n",
      "pred_index=  183\n",
      "pred_index=  184\n",
      "pred_index=  185\n",
      "pred_index=  186\n",
      "pred_index=  187\n",
      "pred_index=  188\n",
      "pred_index=  189\n",
      "pred_index=  190\n",
      "pred_index=  191\n",
      "pred_index=  192\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  193\n",
      "pred_index=  194\n",
      "pred_index=  195\n",
      "pred_index=  196\n",
      "pred_index=  197\n",
      "pred_index=  198\n",
      "pred_index=  199\n",
      "pred_index=  200\n",
      "pred_index=  201\n",
      "pred_index=  202\n",
      "pred_index=  203\n",
      "pred_index=  204\n",
      "pred_index=  205\n",
      "pred_index=  206\n",
      "pred_index=  207\n",
      "pred_index=  208\n",
      "pred_index=  209\n",
      "pred_index=  210\n",
      "pred_index=  211\n",
      "pred_index=  212\n",
      "pred_index=  213\n",
      "pred_index=  214\n",
      "pred_index=  215\n",
      "pred_index=  216\n",
      "pred_index=  217\n",
      "pred_index=  218\n",
      "pred_index=  219\n",
      "pred_index=  220\n",
      "pred_index=  221\n",
      "pred_index=  222\n",
      "pred_index=  223\n",
      "pred_index=  224\n",
      "4/4 [==============================] - 2s 492ms/step\n",
      "pred_index=  225\n",
      "pred_index=  226\n",
      "pred_index=  227\n",
      "pred_index=  228\n",
      "pred_index=  229\n",
      "pred_index=  230\n",
      "pred_index=  231\n",
      "pred_index=  232\n",
      "pred_index=  233\n",
      "pred_index=  234\n",
      "pred_index=  235\n",
      "pred_index=  236\n",
      "pred_index=  237\n",
      "pred_index=  238\n",
      "pred_index=  239\n",
      "pred_index=  240\n",
      "pred_index=  241\n",
      "pred_index=  242\n",
      "pred_index=  243\n",
      "pred_index=  244\n",
      "pred_index=  245\n",
      "pred_index=  246\n",
      "pred_index=  247\n",
      "pred_index=  248\n",
      "pred_index=  249\n",
      "pred_index=  250\n",
      "pred_index=  251\n",
      "pred_index=  252\n",
      "pred_index=  253\n",
      "pred_index=  254\n",
      "pred_index=  255\n",
      "pred_index=  256\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  257\n",
      "pred_index=  258\n",
      "pred_index=  259\n",
      "pred_index=  260\n",
      "pred_index=  261\n",
      "pred_index=  262\n",
      "pred_index=  263\n",
      "pred_index=  264\n",
      "pred_index=  265\n",
      "pred_index=  266\n",
      "pred_index=  267\n",
      "pred_index=  268\n",
      "pred_index=  269\n",
      "pred_index=  270\n",
      "pred_index=  271\n",
      "pred_index=  272\n",
      "pred_index=  273\n",
      "pred_index=  274\n",
      "pred_index=  275\n",
      "pred_index=  276\n",
      "pred_index=  277\n",
      "pred_index=  278\n",
      "pred_index=  279\n",
      "pred_index=  280\n",
      "pred_index=  281\n",
      "pred_index=  282\n",
      "pred_index=  283\n",
      "pred_index=  284\n",
      "pred_index=  285\n",
      "pred_index=  286\n",
      "pred_index=  287\n",
      "pred_index=  288\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  289\n",
      "pred_index=  290\n",
      "pred_index=  291\n",
      "pred_index=  292\n",
      "pred_index=  293\n",
      "pred_index=  294\n",
      "pred_index=  295\n",
      "pred_index=  296\n",
      "pred_index=  297\n",
      "pred_index=  298\n",
      "pred_index=  299\n",
      "pred_index=  300\n",
      "pred_index=  301\n",
      "pred_index=  302\n",
      "pred_index=  303\n",
      "pred_index=  304\n",
      "pred_index=  305\n",
      "pred_index=  306\n",
      "pred_index=  307\n",
      "pred_index=  308\n",
      "pred_index=  309\n",
      "pred_index=  310\n",
      "pred_index=  311\n",
      "pred_index=  312\n",
      "pred_index=  313\n",
      "pred_index=  314\n",
      "pred_index=  315\n",
      "pred_index=  316\n",
      "pred_index=  317\n",
      "pred_index=  318\n",
      "pred_index=  319\n",
      "pred_index=  320\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  321\n",
      "pred_index=  322\n",
      "pred_index=  323\n",
      "pred_index=  324\n",
      "pred_index=  325\n",
      "pred_index=  326\n",
      "pred_index=  327\n",
      "pred_index=  328\n",
      "pred_index=  329\n",
      "pred_index=  330\n",
      "pred_index=  331\n",
      "pred_index=  332\n",
      "pred_index=  333\n",
      "pred_index=  334\n",
      "pred_index=  335\n",
      "pred_index=  336\n",
      "pred_index=  337\n",
      "pred_index=  338\n",
      "pred_index=  339\n",
      "pred_index=  340\n",
      "pred_index=  341\n",
      "pred_index=  342\n",
      "pred_index=  343\n",
      "pred_index=  344\n",
      "pred_index=  345\n",
      "pred_index=  346\n",
      "pred_index=  347\n",
      "pred_index=  348\n",
      "pred_index=  349\n",
      "pred_index=  350\n",
      "pred_index=  351\n",
      "pred_index=  352\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  353\n",
      "pred_index=  354\n",
      "pred_index=  355\n",
      "pred_index=  356\n",
      "pred_index=  357\n",
      "pred_index=  358\n",
      "pred_index=  359\n",
      "pred_index=  360\n",
      "pred_index=  361\n",
      "pred_index=  362\n",
      "pred_index=  363\n",
      "pred_index=  364\n",
      "pred_index=  365\n",
      "pred_index=  366\n",
      "pred_index=  367\n",
      "pred_index=  368\n",
      "pred_index=  369\n",
      "pred_index=  370\n",
      "pred_index=  371\n",
      "pred_index=  372\n",
      "pred_index=  373\n",
      "pred_index=  374\n",
      "pred_index=  375\n",
      "pred_index=  376\n",
      "pred_index=  377\n",
      "pred_index=  378\n",
      "pred_index=  379\n",
      "pred_index=  380\n",
      "pred_index=  381\n",
      "pred_index=  382\n",
      "pred_index=  383\n",
      "pred_index=  384\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  385\n",
      "pred_index=  386\n",
      "pred_index=  387\n",
      "pred_index=  388\n",
      "pred_index=  389\n",
      "pred_index=  390\n",
      "pred_index=  391\n",
      "pred_index=  392\n",
      "pred_index=  393\n",
      "pred_index=  394\n",
      "pred_index=  395\n",
      "pred_index=  396\n",
      "pred_index=  397\n",
      "pred_index=  398\n",
      "pred_index=  399\n",
      "pred_index=  400\n",
      "pred_index=  401\n",
      "pred_index=  402\n",
      "pred_index=  403\n",
      "pred_index=  404\n",
      "pred_index=  405\n",
      "pred_index=  406\n",
      "pred_index=  407\n",
      "pred_index=  408\n",
      "pred_index=  409\n",
      "pred_index=  410\n",
      "pred_index=  411\n",
      "pred_index=  412\n",
      "pred_index=  413\n",
      "pred_index=  414\n",
      "pred_index=  415\n",
      "pred_index=  416\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  417\n",
      "pred_index=  418\n",
      "pred_index=  419\n",
      "pred_index=  420\n",
      "pred_index=  421\n",
      "pred_index=  422\n",
      "pred_index=  423\n",
      "pred_index=  424\n",
      "pred_index=  425\n",
      "pred_index=  426\n",
      "pred_index=  427\n",
      "pred_index=  428\n",
      "pred_index=  429\n",
      "pred_index=  430\n",
      "pred_index=  431\n",
      "pred_index=  432\n",
      "pred_index=  433\n",
      "pred_index=  434\n",
      "pred_index=  435\n",
      "pred_index=  436\n",
      "pred_index=  437\n",
      "pred_index=  438\n",
      "pred_index=  439\n",
      "pred_index=  440\n",
      "pred_index=  441\n",
      "pred_index=  442\n",
      "pred_index=  443\n",
      "pred_index=  444\n",
      "pred_index=  445\n",
      "pred_index=  446\n",
      "pred_index=  447\n",
      "pred_index=  448\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  449\n",
      "pred_index=  450\n",
      "pred_index=  451\n",
      "pred_index=  452\n",
      "pred_index=  453\n",
      "pred_index=  454\n",
      "pred_index=  455\n",
      "pred_index=  456\n",
      "pred_index=  457\n",
      "pred_index=  458\n",
      "pred_index=  459\n",
      "pred_index=  460\n",
      "pred_index=  461\n",
      "pred_index=  462\n",
      "pred_index=  463\n",
      "pred_index=  464\n",
      "pred_index=  465\n",
      "pred_index=  466\n",
      "pred_index=  467\n",
      "pred_index=  468\n",
      "pred_index=  469\n",
      "pred_index=  470\n",
      "pred_index=  471\n",
      "pred_index=  472\n",
      "pred_index=  473\n",
      "pred_index=  474\n",
      "pred_index=  475\n",
      "pred_index=  476\n",
      "pred_index=  477\n",
      "pred_index=  478\n",
      "pred_index=  479\n",
      "pred_index=  480\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  481\n",
      "pred_index=  482\n",
      "pred_index=  483\n",
      "pred_index=  484\n",
      "pred_index=  485\n",
      "pred_index=  486\n",
      "pred_index=  487\n",
      "pred_index=  488\n",
      "pred_index=  489\n",
      "pred_index=  490\n",
      "pred_index=  491\n",
      "pred_index=  492\n",
      "pred_index=  493\n",
      "pred_index=  494\n",
      "pred_index=  495\n",
      "pred_index=  496\n",
      "pred_index=  497\n",
      "pred_index=  498\n",
      "pred_index=  499\n",
      "pred_index=  500\n",
      "pred_index=  501\n",
      "pred_index=  502\n",
      "pred_index=  503\n",
      "pred_index=  504\n",
      "pred_index=  505\n",
      "pred_index=  506\n",
      "pred_index=  507\n",
      "pred_index=  508\n",
      "pred_index=  509\n",
      "pred_index=  510\n",
      "pred_index=  511\n",
      "pred_index=  512\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  513\n",
      "pred_index=  514\n",
      "pred_index=  515\n",
      "pred_index=  516\n",
      "pred_index=  517\n",
      "pred_index=  518\n",
      "pred_index=  519\n",
      "pred_index=  520\n",
      "pred_index=  521\n",
      "pred_index=  522\n",
      "pred_index=  523\n",
      "pred_index=  524\n",
      "pred_index=  525\n",
      "pred_index=  526\n",
      "pred_index=  527\n",
      "pred_index=  528\n",
      "pred_index=  529\n",
      "pred_index=  530\n",
      "pred_index=  531\n",
      "pred_index=  532\n",
      "pred_index=  533\n",
      "pred_index=  534\n",
      "pred_index=  535\n",
      "pred_index=  536\n",
      "pred_index=  537\n",
      "pred_index=  538\n",
      "pred_index=  539\n",
      "pred_index=  540\n",
      "pred_index=  541\n",
      "pred_index=  542\n",
      "pred_index=  543\n",
      "pred_index=  544\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  545\n",
      "pred_index=  546\n",
      "pred_index=  547\n",
      "pred_index=  548\n",
      "pred_index=  549\n",
      "pred_index=  550\n",
      "pred_index=  551\n",
      "pred_index=  552\n",
      "pred_index=  553\n",
      "pred_index=  554\n",
      "pred_index=  555\n",
      "pred_index=  556\n",
      "pred_index=  557\n",
      "pred_index=  558\n",
      "pred_index=  559\n",
      "pred_index=  560\n",
      "pred_index=  561\n",
      "pred_index=  562\n",
      "pred_index=  563\n",
      "pred_index=  564\n",
      "pred_index=  565\n",
      "pred_index=  566\n",
      "pred_index=  567\n",
      "pred_index=  568\n",
      "pred_index=  569\n",
      "pred_index=  570\n",
      "pred_index=  571\n",
      "pred_index=  572\n",
      "pred_index=  573\n",
      "pred_index=  574\n",
      "pred_index=  575\n",
      "pred_index=  576\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  577\n",
      "pred_index=  578\n",
      "pred_index=  579\n",
      "pred_index=  580\n",
      "pred_index=  581\n",
      "pred_index=  582\n",
      "pred_index=  583\n",
      "pred_index=  584\n",
      "pred_index=  585\n",
      "pred_index=  586\n",
      "pred_index=  587\n",
      "pred_index=  588\n",
      "pred_index=  589\n",
      "pred_index=  590\n",
      "pred_index=  591\n",
      "pred_index=  592\n",
      "pred_index=  593\n",
      "pred_index=  594\n",
      "pred_index=  595\n",
      "pred_index=  596\n",
      "pred_index=  597\n",
      "pred_index=  598\n",
      "pred_index=  599\n",
      "pred_index=  600\n",
      "pred_index=  601\n",
      "pred_index=  602\n",
      "pred_index=  603\n",
      "pred_index=  604\n",
      "pred_index=  605\n",
      "pred_index=  606\n",
      "pred_index=  607\n",
      "pred_index=  608\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  609\n",
      "pred_index=  610\n",
      "pred_index=  611\n",
      "pred_index=  612\n",
      "pred_index=  613\n",
      "pred_index=  614\n",
      "pred_index=  615\n",
      "pred_index=  616\n",
      "pred_index=  617\n",
      "pred_index=  618\n",
      "pred_index=  619\n",
      "pred_index=  620\n",
      "pred_index=  621\n",
      "pred_index=  622\n",
      "pred_index=  623\n",
      "pred_index=  624\n",
      "pred_index=  625\n",
      "pred_index=  626\n",
      "pred_index=  627\n",
      "pred_index=  628\n",
      "pred_index=  629\n",
      "pred_index=  630\n",
      "pred_index=  631\n",
      "pred_index=  632\n",
      "pred_index=  633\n",
      "pred_index=  634\n",
      "pred_index=  635\n",
      "pred_index=  636\n",
      "pred_index=  637\n",
      "pred_index=  638\n",
      "pred_index=  639\n",
      "pred_index=  640\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  641\n",
      "pred_index=  642\n",
      "pred_index=  643\n",
      "pred_index=  644\n",
      "pred_index=  645\n",
      "pred_index=  646\n",
      "pred_index=  647\n",
      "pred_index=  648\n",
      "pred_index=  649\n",
      "pred_index=  650\n",
      "pred_index=  651\n",
      "pred_index=  652\n",
      "pred_index=  653\n",
      "pred_index=  654\n",
      "pred_index=  655\n",
      "pred_index=  656\n",
      "pred_index=  657\n",
      "pred_index=  658\n",
      "pred_index=  659\n",
      "pred_index=  660\n",
      "pred_index=  661\n",
      "pred_index=  662\n",
      "pred_index=  663\n",
      "pred_index=  664\n",
      "pred_index=  665\n",
      "pred_index=  666\n",
      "pred_index=  667\n",
      "pred_index=  668\n",
      "pred_index=  669\n",
      "pred_index=  670\n",
      "pred_index=  671\n",
      "pred_index=  672\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  673\n",
      "pred_index=  674\n",
      "pred_index=  675\n",
      "pred_index=  676\n",
      "pred_index=  677\n",
      "pred_index=  678\n",
      "pred_index=  679\n",
      "pred_index=  680\n",
      "pred_index=  681\n",
      "pred_index=  682\n",
      "pred_index=  683\n",
      "pred_index=  684\n",
      "pred_index=  685\n",
      "pred_index=  686\n",
      "pred_index=  687\n",
      "pred_index=  688\n",
      "pred_index=  689\n",
      "pred_index=  690\n",
      "pred_index=  691\n",
      "pred_index=  692\n",
      "pred_index=  693\n",
      "pred_index=  694\n",
      "pred_index=  695\n",
      "pred_index=  696\n",
      "pred_index=  697\n",
      "pred_index=  698\n",
      "pred_index=  699\n",
      "pred_index=  700\n",
      "pred_index=  701\n",
      "pred_index=  702\n",
      "pred_index=  703\n",
      "pred_index=  704\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  705\n",
      "pred_index=  706\n",
      "pred_index=  707\n",
      "pred_index=  708\n",
      "pred_index=  709\n",
      "pred_index=  710\n",
      "pred_index=  711\n",
      "pred_index=  712\n",
      "pred_index=  713\n",
      "pred_index=  714\n",
      "pred_index=  715\n",
      "pred_index=  716\n",
      "pred_index=  717\n",
      "pred_index=  718\n",
      "pred_index=  719\n",
      "pred_index=  720\n",
      "pred_index=  721\n",
      "pred_index=  722\n",
      "pred_index=  723\n",
      "pred_index=  724\n",
      "pred_index=  725\n",
      "pred_index=  726\n",
      "pred_index=  727\n",
      "pred_index=  728\n",
      "pred_index=  729\n",
      "pred_index=  730\n",
      "pred_index=  731\n",
      "pred_index=  732\n",
      "pred_index=  733\n",
      "pred_index=  734\n",
      "pred_index=  735\n",
      "pred_index=  736\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  737\n",
      "pred_index=  738\n",
      "pred_index=  739\n",
      "pred_index=  740\n",
      "pred_index=  741\n",
      "pred_index=  742\n",
      "pred_index=  743\n",
      "pred_index=  744\n",
      "pred_index=  745\n",
      "pred_index=  746\n",
      "pred_index=  747\n",
      "pred_index=  748\n",
      "pred_index=  749\n",
      "pred_index=  750\n",
      "pred_index=  751\n",
      "pred_index=  752\n",
      "pred_index=  753\n",
      "pred_index=  754\n",
      "pred_index=  755\n",
      "pred_index=  756\n",
      "pred_index=  757\n",
      "pred_index=  758\n",
      "pred_index=  759\n",
      "pred_index=  760\n",
      "pred_index=  761\n",
      "pred_index=  762\n",
      "pred_index=  763\n",
      "pred_index=  764\n",
      "pred_index=  765\n",
      "pred_index=  766\n",
      "pred_index=  767\n",
      "pred_index=  768\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  769\n",
      "pred_index=  770\n",
      "pred_index=  771\n",
      "pred_index=  772\n",
      "pred_index=  773\n",
      "pred_index=  774\n",
      "pred_index=  775\n",
      "pred_index=  776\n",
      "pred_index=  777\n",
      "pred_index=  778\n",
      "pred_index=  779\n",
      "pred_index=  780\n",
      "pred_index=  781\n",
      "pred_index=  782\n",
      "pred_index=  783\n",
      "pred_index=  784\n",
      "pred_index=  785\n",
      "pred_index=  786\n",
      "pred_index=  787\n",
      "pred_index=  788\n",
      "pred_index=  789\n",
      "pred_index=  790\n",
      "pred_index=  791\n",
      "pred_index=  792\n",
      "pred_index=  793\n",
      "pred_index=  794\n",
      "pred_index=  795\n",
      "pred_index=  796\n",
      "pred_index=  797\n",
      "pred_index=  798\n",
      "pred_index=  799\n",
      "pred_index=  800\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  801\n",
      "pred_index=  802\n",
      "pred_index=  803\n",
      "pred_index=  804\n",
      "pred_index=  805\n",
      "pred_index=  806\n",
      "pred_index=  807\n",
      "pred_index=  808\n",
      "pred_index=  809\n",
      "pred_index=  810\n",
      "pred_index=  811\n",
      "pred_index=  812\n",
      "pred_index=  813\n",
      "pred_index=  814\n",
      "pred_index=  815\n",
      "pred_index=  816\n",
      "pred_index=  817\n",
      "pred_index=  818\n",
      "pred_index=  819\n",
      "pred_index=  820\n",
      "pred_index=  821\n",
      "pred_index=  822\n",
      "pred_index=  823\n",
      "pred_index=  824\n",
      "pred_index=  825\n",
      "pred_index=  826\n",
      "pred_index=  827\n",
      "pred_index=  828\n",
      "pred_index=  829\n",
      "pred_index=  830\n",
      "pred_index=  831\n",
      "pred_index=  832\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  833\n",
      "pred_index=  834\n",
      "pred_index=  835\n",
      "pred_index=  836\n",
      "pred_index=  837\n",
      "pred_index=  838\n",
      "pred_index=  839\n",
      "pred_index=  840\n",
      "pred_index=  841\n",
      "pred_index=  842\n",
      "pred_index=  843\n",
      "pred_index=  844\n",
      "pred_index=  845\n",
      "pred_index=  846\n",
      "pred_index=  847\n",
      "pred_index=  848\n",
      "pred_index=  849\n",
      "pred_index=  850\n",
      "pred_index=  851\n",
      "pred_index=  852\n",
      "pred_index=  853\n",
      "pred_index=  854\n",
      "pred_index=  855\n",
      "pred_index=  856\n",
      "pred_index=  857\n",
      "pred_index=  858\n",
      "pred_index=  859\n",
      "pred_index=  860\n",
      "pred_index=  861\n",
      "pred_index=  862\n",
      "pred_index=  863\n",
      "pred_index=  864\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  865\n",
      "pred_index=  866\n",
      "pred_index=  867\n",
      "pred_index=  868\n",
      "pred_index=  869\n",
      "pred_index=  870\n",
      "pred_index=  871\n",
      "pred_index=  872\n",
      "pred_index=  873\n",
      "pred_index=  874\n",
      "pred_index=  875\n",
      "pred_index=  876\n",
      "pred_index=  877\n",
      "pred_index=  878\n",
      "pred_index=  879\n",
      "pred_index=  880\n",
      "pred_index=  881\n",
      "pred_index=  882\n",
      "pred_index=  883\n",
      "pred_index=  884\n",
      "pred_index=  885\n",
      "pred_index=  886\n",
      "pred_index=  887\n",
      "pred_index=  888\n",
      "pred_index=  889\n",
      "pred_index=  890\n",
      "pred_index=  891\n",
      "pred_index=  892\n",
      "pred_index=  893\n",
      "pred_index=  894\n",
      "pred_index=  895\n",
      "pred_index=  896\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  897\n",
      "pred_index=  898\n",
      "pred_index=  899\n",
      "pred_index=  900\n",
      "pred_index=  901\n",
      "pred_index=  902\n",
      "pred_index=  903\n",
      "pred_index=  904\n",
      "pred_index=  905\n",
      "pred_index=  906\n",
      "pred_index=  907\n",
      "pred_index=  908\n",
      "pred_index=  909\n",
      "pred_index=  910\n",
      "pred_index=  911\n",
      "pred_index=  912\n",
      "pred_index=  913\n",
      "pred_index=  914\n",
      "pred_index=  915\n",
      "pred_index=  916\n",
      "pred_index=  917\n",
      "pred_index=  918\n",
      "pred_index=  919\n",
      "pred_index=  920\n",
      "pred_index=  921\n",
      "pred_index=  922\n",
      "pred_index=  923\n",
      "pred_index=  924\n",
      "pred_index=  925\n",
      "pred_index=  926\n",
      "pred_index=  927\n",
      "pred_index=  928\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  929\n",
      "pred_index=  930\n",
      "pred_index=  931\n",
      "pred_index=  932\n",
      "pred_index=  933\n",
      "pred_index=  934\n",
      "pred_index=  935\n",
      "pred_index=  936\n",
      "pred_index=  937\n",
      "pred_index=  938\n",
      "pred_index=  939\n",
      "pred_index=  940\n",
      "pred_index=  941\n",
      "pred_index=  942\n",
      "pred_index=  943\n",
      "pred_index=  944\n",
      "pred_index=  945\n",
      "pred_index=  946\n",
      "pred_index=  947\n",
      "pred_index=  948\n",
      "pred_index=  949\n",
      "pred_index=  950\n",
      "pred_index=  951\n",
      "pred_index=  952\n",
      "pred_index=  953\n",
      "pred_index=  954\n",
      "pred_index=  955\n",
      "pred_index=  956\n",
      "pred_index=  957\n",
      "pred_index=  958\n",
      "pred_index=  959\n",
      "pred_index=  960\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  961\n",
      "pred_index=  962\n",
      "pred_index=  963\n",
      "pred_index=  964\n",
      "pred_index=  965\n",
      "pred_index=  966\n",
      "pred_index=  967\n",
      "pred_index=  968\n",
      "pred_index=  969\n",
      "pred_index=  970\n",
      "pred_index=  971\n",
      "pred_index=  972\n",
      "pred_index=  973\n",
      "pred_index=  974\n",
      "pred_index=  975\n",
      "pred_index=  976\n",
      "pred_index=  977\n",
      "pred_index=  978\n",
      "pred_index=  979\n",
      "pred_index=  980\n",
      "pred_index=  981\n",
      "pred_index=  982\n",
      "pred_index=  983\n",
      "pred_index=  984\n",
      "pred_index=  985\n",
      "pred_index=  986\n",
      "pred_index=  987\n",
      "pred_index=  988\n",
      "pred_index=  989\n",
      "pred_index=  990\n",
      "pred_index=  991\n",
      "pred_index=  992\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  993\n",
      "pred_index=  994\n",
      "pred_index=  995\n",
      "pred_index=  996\n",
      "pred_index=  997\n",
      "pred_index=  998\n",
      "pred_index=  999\n",
      "pred_index=  1000\n",
      "pred_index=  1001\n",
      "pred_index=  1002\n",
      "pred_index=  1003\n",
      "pred_index=  1004\n",
      "pred_index=  1005\n",
      "pred_index=  1006\n",
      "pred_index=  1007\n",
      "pred_index=  1008\n",
      "pred_index=  1009\n",
      "pred_index=  1010\n",
      "pred_index=  1011\n",
      "pred_index=  1012\n",
      "pred_index=  1013\n",
      "pred_index=  1014\n",
      "pred_index=  1015\n",
      "pred_index=  1016\n",
      "pred_index=  1017\n",
      "pred_index=  1018\n",
      "pred_index=  1019\n",
      "pred_index=  1020\n",
      "pred_index=  1021\n",
      "pred_index=  1022\n",
      "pred_index=  1023\n",
      "pred_index=  1024\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1025\n",
      "pred_index=  1026\n",
      "pred_index=  1027\n",
      "pred_index=  1028\n",
      "pred_index=  1029\n",
      "pred_index=  1030\n",
      "pred_index=  1031\n",
      "pred_index=  1032\n",
      "pred_index=  1033\n",
      "pred_index=  1034\n",
      "pred_index=  1035\n",
      "pred_index=  1036\n",
      "pred_index=  1037\n",
      "pred_index=  1038\n",
      "pred_index=  1039\n",
      "pred_index=  1040\n",
      "pred_index=  1041\n",
      "pred_index=  1042\n",
      "pred_index=  1043\n",
      "pred_index=  1044\n",
      "pred_index=  1045\n",
      "pred_index=  1046\n",
      "pred_index=  1047\n",
      "pred_index=  1048\n",
      "pred_index=  1049\n",
      "pred_index=  1050\n",
      "pred_index=  1051\n",
      "pred_index=  1052\n",
      "pred_index=  1053\n",
      "pred_index=  1054\n",
      "pred_index=  1055\n",
      "pred_index=  1056\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1057\n",
      "pred_index=  1058\n",
      "pred_index=  1059\n",
      "pred_index=  1060\n",
      "pred_index=  1061\n",
      "pred_index=  1062\n",
      "pred_index=  1063\n",
      "pred_index=  1064\n",
      "pred_index=  1065\n",
      "pred_index=  1066\n",
      "pred_index=  1067\n",
      "pred_index=  1068\n",
      "pred_index=  1069\n",
      "pred_index=  1070\n",
      "pred_index=  1071\n",
      "pred_index=  1072\n",
      "pred_index=  1073\n",
      "pred_index=  1074\n",
      "pred_index=  1075\n",
      "pred_index=  1076\n",
      "pred_index=  1077\n",
      "pred_index=  1078\n",
      "pred_index=  1079\n",
      "pred_index=  1080\n",
      "pred_index=  1081\n",
      "pred_index=  1082\n",
      "pred_index=  1083\n",
      "pred_index=  1084\n",
      "pred_index=  1085\n",
      "pred_index=  1086\n",
      "pred_index=  1087\n",
      "pred_index=  1088\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1089\n",
      "pred_index=  1090\n",
      "pred_index=  1091\n",
      "pred_index=  1092\n",
      "pred_index=  1093\n",
      "pred_index=  1094\n",
      "pred_index=  1095\n",
      "pred_index=  1096\n",
      "pred_index=  1097\n",
      "pred_index=  1098\n",
      "pred_index=  1099\n",
      "pred_index=  1100\n",
      "pred_index=  1101\n",
      "pred_index=  1102\n",
      "pred_index=  1103\n",
      "pred_index=  1104\n",
      "pred_index=  1105\n",
      "pred_index=  1106\n",
      "pred_index=  1107\n",
      "pred_index=  1108\n",
      "pred_index=  1109\n",
      "pred_index=  1110\n",
      "pred_index=  1111\n",
      "pred_index=  1112\n",
      "pred_index=  1113\n",
      "pred_index=  1114\n",
      "pred_index=  1115\n",
      "pred_index=  1116\n",
      "pred_index=  1117\n",
      "pred_index=  1118\n",
      "pred_index=  1119\n",
      "pred_index=  1120\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1121\n",
      "pred_index=  1122\n",
      "pred_index=  1123\n",
      "pred_index=  1124\n",
      "pred_index=  1125\n",
      "pred_index=  1126\n",
      "pred_index=  1127\n",
      "pred_index=  1128\n",
      "pred_index=  1129\n",
      "pred_index=  1130\n",
      "pred_index=  1131\n",
      "pred_index=  1132\n",
      "pred_index=  1133\n",
      "pred_index=  1134\n",
      "pred_index=  1135\n",
      "pred_index=  1136\n",
      "pred_index=  1137\n",
      "pred_index=  1138\n",
      "pred_index=  1139\n",
      "pred_index=  1140\n",
      "pred_index=  1141\n",
      "pred_index=  1142\n",
      "pred_index=  1143\n",
      "pred_index=  1144\n",
      "pred_index=  1145\n",
      "pred_index=  1146\n",
      "pred_index=  1147\n",
      "pred_index=  1148\n",
      "pred_index=  1149\n",
      "pred_index=  1150\n",
      "pred_index=  1151\n",
      "pred_index=  1152\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1153\n",
      "pred_index=  1154\n",
      "pred_index=  1155\n",
      "pred_index=  1156\n",
      "pred_index=  1157\n",
      "pred_index=  1158\n",
      "pred_index=  1159\n",
      "pred_index=  1160\n",
      "pred_index=  1161\n",
      "pred_index=  1162\n",
      "pred_index=  1163\n",
      "pred_index=  1164\n",
      "pred_index=  1165\n",
      "pred_index=  1166\n",
      "pred_index=  1167\n",
      "pred_index=  1168\n",
      "pred_index=  1169\n",
      "pred_index=  1170\n",
      "pred_index=  1171\n",
      "pred_index=  1172\n",
      "pred_index=  1173\n",
      "pred_index=  1174\n",
      "pred_index=  1175\n",
      "pred_index=  1176\n",
      "pred_index=  1177\n",
      "pred_index=  1178\n",
      "pred_index=  1179\n",
      "pred_index=  1180\n",
      "pred_index=  1181\n",
      "pred_index=  1182\n",
      "pred_index=  1183\n",
      "pred_index=  1184\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1185\n",
      "pred_index=  1186\n",
      "pred_index=  1187\n",
      "pred_index=  1188\n",
      "pred_index=  1189\n",
      "pred_index=  1190\n",
      "pred_index=  1191\n",
      "pred_index=  1192\n",
      "pred_index=  1193\n",
      "pred_index=  1194\n",
      "pred_index=  1195\n",
      "pred_index=  1196\n",
      "pred_index=  1197\n",
      "pred_index=  1198\n",
      "pred_index=  1199\n",
      "pred_index=  1200\n",
      "pred_index=  1201\n",
      "pred_index=  1202\n",
      "pred_index=  1203\n",
      "pred_index=  1204\n",
      "pred_index=  1205\n",
      "pred_index=  1206\n",
      "pred_index=  1207\n",
      "pred_index=  1208\n",
      "pred_index=  1209\n",
      "pred_index=  1210\n",
      "pred_index=  1211\n",
      "pred_index=  1212\n",
      "pred_index=  1213\n",
      "pred_index=  1214\n",
      "pred_index=  1215\n",
      "pred_index=  1216\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1217\n",
      "pred_index=  1218\n",
      "pred_index=  1219\n",
      "pred_index=  1220\n",
      "pred_index=  1221\n",
      "pred_index=  1222\n",
      "pred_index=  1223\n",
      "pred_index=  1224\n",
      "pred_index=  1225\n",
      "pred_index=  1226\n",
      "pred_index=  1227\n",
      "pred_index=  1228\n",
      "pred_index=  1229\n",
      "pred_index=  1230\n",
      "pred_index=  1231\n",
      "pred_index=  1232\n",
      "pred_index=  1233\n",
      "pred_index=  1234\n",
      "pred_index=  1235\n",
      "pred_index=  1236\n",
      "pred_index=  1237\n",
      "pred_index=  1238\n",
      "pred_index=  1239\n",
      "pred_index=  1240\n",
      "pred_index=  1241\n",
      "pred_index=  1242\n",
      "pred_index=  1243\n",
      "pred_index=  1244\n",
      "pred_index=  1245\n",
      "pred_index=  1246\n",
      "pred_index=  1247\n",
      "pred_index=  1248\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1249\n",
      "pred_index=  1250\n",
      "pred_index=  1251\n",
      "pred_index=  1252\n",
      "pred_index=  1253\n",
      "pred_index=  1254\n",
      "pred_index=  1255\n",
      "pred_index=  1256\n",
      "pred_index=  1257\n",
      "pred_index=  1258\n",
      "pred_index=  1259\n",
      "pred_index=  1260\n",
      "pred_index=  1261\n",
      "pred_index=  1262\n",
      "pred_index=  1263\n",
      "pred_index=  1264\n",
      "pred_index=  1265\n",
      "pred_index=  1266\n",
      "pred_index=  1267\n",
      "pred_index=  1268\n",
      "pred_index=  1269\n",
      "pred_index=  1270\n",
      "pred_index=  1271\n",
      "pred_index=  1272\n",
      "pred_index=  1273\n",
      "pred_index=  1274\n",
      "pred_index=  1275\n",
      "pred_index=  1276\n",
      "pred_index=  1277\n",
      "pred_index=  1278\n",
      "pred_index=  1279\n",
      "pred_index=  1280\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1281\n",
      "pred_index=  1282\n",
      "pred_index=  1283\n",
      "pred_index=  1284\n",
      "pred_index=  1285\n",
      "pred_index=  1286\n",
      "pred_index=  1287\n",
      "pred_index=  1288\n",
      "pred_index=  1289\n",
      "pred_index=  1290\n",
      "pred_index=  1291\n",
      "pred_index=  1292\n",
      "pred_index=  1293\n",
      "pred_index=  1294\n",
      "pred_index=  1295\n",
      "pred_index=  1296\n",
      "pred_index=  1297\n",
      "pred_index=  1298\n",
      "pred_index=  1299\n",
      "pred_index=  1300\n",
      "pred_index=  1301\n",
      "pred_index=  1302\n",
      "pred_index=  1303\n",
      "pred_index=  1304\n",
      "pred_index=  1305\n",
      "pred_index=  1306\n",
      "pred_index=  1307\n",
      "pred_index=  1308\n",
      "pred_index=  1309\n",
      "pred_index=  1310\n",
      "pred_index=  1311\n",
      "pred_index=  1312\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1313\n",
      "pred_index=  1314\n",
      "pred_index=  1315\n",
      "pred_index=  1316\n",
      "pred_index=  1317\n",
      "pred_index=  1318\n",
      "pred_index=  1319\n",
      "pred_index=  1320\n",
      "pred_index=  1321\n",
      "pred_index=  1322\n",
      "pred_index=  1323\n",
      "pred_index=  1324\n",
      "pred_index=  1325\n",
      "pred_index=  1326\n",
      "pred_index=  1327\n",
      "pred_index=  1328\n",
      "pred_index=  1329\n",
      "pred_index=  1330\n",
      "pred_index=  1331\n",
      "pred_index=  1332\n",
      "pred_index=  1333\n",
      "pred_index=  1334\n",
      "pred_index=  1335\n",
      "pred_index=  1336\n",
      "pred_index=  1337\n",
      "pred_index=  1338\n",
      "pred_index=  1339\n",
      "pred_index=  1340\n",
      "pred_index=  1341\n",
      "pred_index=  1342\n",
      "pred_index=  1343\n",
      "pred_index=  1344\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1345\n",
      "pred_index=  1346\n",
      "pred_index=  1347\n",
      "pred_index=  1348\n",
      "pred_index=  1349\n",
      "pred_index=  1350\n",
      "pred_index=  1351\n",
      "pred_index=  1352\n",
      "pred_index=  1353\n",
      "pred_index=  1354\n",
      "pred_index=  1355\n",
      "pred_index=  1356\n",
      "pred_index=  1357\n",
      "pred_index=  1358\n",
      "pred_index=  1359\n",
      "pred_index=  1360\n",
      "pred_index=  1361\n",
      "pred_index=  1362\n",
      "pred_index=  1363\n",
      "pred_index=  1364\n",
      "pred_index=  1365\n",
      "pred_index=  1366\n",
      "pred_index=  1367\n",
      "pred_index=  1368\n",
      "pred_index=  1369\n",
      "pred_index=  1370\n",
      "pred_index=  1371\n",
      "pred_index=  1372\n",
      "pred_index=  1373\n",
      "pred_index=  1374\n",
      "pred_index=  1375\n",
      "pred_index=  1376\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1377\n",
      "pred_index=  1378\n",
      "pred_index=  1379\n",
      "pred_index=  1380\n",
      "pred_index=  1381\n",
      "pred_index=  1382\n",
      "pred_index=  1383\n",
      "pred_index=  1384\n",
      "pred_index=  1385\n",
      "pred_index=  1386\n",
      "pred_index=  1387\n",
      "pred_index=  1388\n",
      "pred_index=  1389\n",
      "pred_index=  1390\n",
      "pred_index=  1391\n",
      "pred_index=  1392\n",
      "pred_index=  1393\n",
      "pred_index=  1394\n",
      "pred_index=  1395\n",
      "pred_index=  1396\n",
      "pred_index=  1397\n",
      "pred_index=  1398\n",
      "pred_index=  1399\n",
      "pred_index=  1400\n",
      "pred_index=  1401\n",
      "pred_index=  1402\n",
      "pred_index=  1403\n",
      "pred_index=  1404\n",
      "pred_index=  1405\n",
      "pred_index=  1406\n",
      "pred_index=  1407\n",
      "pred_index=  1408\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1409\n",
      "pred_index=  1410\n",
      "pred_index=  1411\n",
      "pred_index=  1412\n",
      "pred_index=  1413\n",
      "pred_index=  1414\n",
      "pred_index=  1415\n",
      "pred_index=  1416\n",
      "pred_index=  1417\n",
      "pred_index=  1418\n",
      "pred_index=  1419\n",
      "pred_index=  1420\n",
      "pred_index=  1421\n",
      "pred_index=  1422\n",
      "pred_index=  1423\n",
      "pred_index=  1424\n",
      "pred_index=  1425\n",
      "pred_index=  1426\n",
      "pred_index=  1427\n",
      "pred_index=  1428\n",
      "pred_index=  1429\n",
      "pred_index=  1430\n",
      "pred_index=  1431\n",
      "pred_index=  1432\n",
      "pred_index=  1433\n",
      "pred_index=  1434\n",
      "pred_index=  1435\n",
      "pred_index=  1436\n",
      "pred_index=  1437\n",
      "pred_index=  1438\n",
      "pred_index=  1439\n",
      "pred_index=  1440\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1441\n",
      "pred_index=  1442\n",
      "pred_index=  1443\n",
      "pred_index=  1444\n",
      "pred_index=  1445\n",
      "pred_index=  1446\n",
      "pred_index=  1447\n",
      "pred_index=  1448\n",
      "pred_index=  1449\n",
      "pred_index=  1450\n",
      "pred_index=  1451\n",
      "pred_index=  1452\n",
      "pred_index=  1453\n",
      "pred_index=  1454\n",
      "pred_index=  1455\n",
      "pred_index=  1456\n",
      "pred_index=  1457\n",
      "pred_index=  1458\n",
      "pred_index=  1459\n",
      "pred_index=  1460\n",
      "pred_index=  1461\n",
      "pred_index=  1462\n",
      "pred_index=  1463\n",
      "pred_index=  1464\n",
      "pred_index=  1465\n",
      "pred_index=  1466\n",
      "pred_index=  1467\n",
      "pred_index=  1468\n",
      "pred_index=  1469\n",
      "pred_index=  1470\n",
      "pred_index=  1471\n",
      "pred_index=  1472\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1473\n",
      "pred_index=  1474\n",
      "pred_index=  1475\n",
      "pred_index=  1476\n",
      "pred_index=  1477\n",
      "pred_index=  1478\n",
      "pred_index=  1479\n",
      "pred_index=  1480\n",
      "pred_index=  1481\n",
      "pred_index=  1482\n",
      "pred_index=  1483\n",
      "pred_index=  1484\n",
      "pred_index=  1485\n",
      "pred_index=  1486\n",
      "pred_index=  1487\n",
      "pred_index=  1488\n",
      "pred_index=  1489\n",
      "pred_index=  1490\n",
      "pred_index=  1491\n",
      "pred_index=  1492\n",
      "pred_index=  1493\n",
      "pred_index=  1494\n",
      "pred_index=  1495\n",
      "pred_index=  1496\n",
      "pred_index=  1497\n",
      "pred_index=  1498\n",
      "pred_index=  1499\n",
      "pred_index=  1500\n",
      "pred_index=  1501\n",
      "pred_index=  1502\n",
      "pred_index=  1503\n",
      "pred_index=  1504\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1505\n",
      "pred_index=  1506\n",
      "pred_index=  1507\n",
      "pred_index=  1508\n",
      "pred_index=  1509\n",
      "pred_index=  1510\n",
      "pred_index=  1511\n",
      "pred_index=  1512\n",
      "pred_index=  1513\n",
      "pred_index=  1514\n",
      "pred_index=  1515\n",
      "pred_index=  1516\n",
      "pred_index=  1517\n",
      "pred_index=  1518\n",
      "pred_index=  1519\n",
      "pred_index=  1520\n",
      "pred_index=  1521\n",
      "pred_index=  1522\n",
      "pred_index=  1523\n",
      "pred_index=  1524\n",
      "pred_index=  1525\n",
      "pred_index=  1526\n",
      "pred_index=  1527\n",
      "pred_index=  1528\n",
      "pred_index=  1529\n",
      "pred_index=  1530\n",
      "pred_index=  1531\n",
      "pred_index=  1532\n",
      "pred_index=  1533\n",
      "pred_index=  1534\n",
      "pred_index=  1535\n",
      "pred_index=  1536\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1537\n",
      "pred_index=  1538\n",
      "pred_index=  1539\n",
      "pred_index=  1540\n",
      "pred_index=  1541\n",
      "pred_index=  1542\n",
      "pred_index=  1543\n",
      "pred_index=  1544\n",
      "pred_index=  1545\n",
      "pred_index=  1546\n",
      "pred_index=  1547\n",
      "pred_index=  1548\n",
      "pred_index=  1549\n",
      "pred_index=  1550\n",
      "pred_index=  1551\n",
      "pred_index=  1552\n",
      "pred_index=  1553\n",
      "pred_index=  1554\n",
      "pred_index=  1555\n",
      "pred_index=  1556\n",
      "pred_index=  1557\n",
      "pred_index=  1558\n",
      "pred_index=  1559\n",
      "pred_index=  1560\n",
      "pred_index=  1561\n",
      "pred_index=  1562\n",
      "pred_index=  1563\n",
      "pred_index=  1564\n",
      "pred_index=  1565\n",
      "pred_index=  1566\n",
      "pred_index=  1567\n",
      "pred_index=  1568\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1569\n",
      "pred_index=  1570\n",
      "pred_index=  1571\n",
      "pred_index=  1572\n",
      "pred_index=  1573\n",
      "pred_index=  1574\n",
      "pred_index=  1575\n",
      "pred_index=  1576\n",
      "pred_index=  1577\n",
      "pred_index=  1578\n",
      "pred_index=  1579\n",
      "pred_index=  1580\n",
      "pred_index=  1581\n",
      "pred_index=  1582\n",
      "pred_index=  1583\n",
      "pred_index=  1584\n",
      "pred_index=  1585\n",
      "pred_index=  1586\n",
      "pred_index=  1587\n",
      "pred_index=  1588\n",
      "pred_index=  1589\n",
      "pred_index=  1590\n",
      "pred_index=  1591\n",
      "pred_index=  1592\n",
      "pred_index=  1593\n",
      "pred_index=  1594\n",
      "pred_index=  1595\n",
      "pred_index=  1596\n",
      "pred_index=  1597\n",
      "pred_index=  1598\n",
      "pred_index=  1599\n",
      "pred_index=  1600\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1601\n",
      "pred_index=  1602\n",
      "pred_index=  1603\n",
      "pred_index=  1604\n",
      "pred_index=  1605\n",
      "pred_index=  1606\n",
      "pred_index=  1607\n",
      "pred_index=  1608\n",
      "pred_index=  1609\n",
      "pred_index=  1610\n",
      "pred_index=  1611\n",
      "pred_index=  1612\n",
      "pred_index=  1613\n",
      "pred_index=  1614\n",
      "pred_index=  1615\n",
      "pred_index=  1616\n",
      "pred_index=  1617\n",
      "pred_index=  1618\n",
      "pred_index=  1619\n",
      "pred_index=  1620\n",
      "pred_index=  1621\n",
      "pred_index=  1622\n",
      "pred_index=  1623\n",
      "pred_index=  1624\n",
      "pred_index=  1625\n",
      "pred_index=  1626\n",
      "pred_index=  1627\n",
      "pred_index=  1628\n",
      "pred_index=  1629\n",
      "pred_index=  1630\n",
      "pred_index=  1631\n",
      "pred_index=  1632\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1633\n",
      "pred_index=  1634\n",
      "pred_index=  1635\n",
      "pred_index=  1636\n",
      "pred_index=  1637\n",
      "pred_index=  1638\n",
      "pred_index=  1639\n",
      "pred_index=  1640\n",
      "pred_index=  1641\n",
      "pred_index=  1642\n",
      "pred_index=  1643\n",
      "pred_index=  1644\n",
      "pred_index=  1645\n",
      "pred_index=  1646\n",
      "pred_index=  1647\n",
      "pred_index=  1648\n",
      "pred_index=  1649\n",
      "pred_index=  1650\n",
      "pred_index=  1651\n",
      "pred_index=  1652\n",
      "pred_index=  1653\n",
      "pred_index=  1654\n",
      "pred_index=  1655\n",
      "pred_index=  1656\n",
      "pred_index=  1657\n",
      "pred_index=  1658\n",
      "pred_index=  1659\n",
      "pred_index=  1660\n",
      "pred_index=  1661\n",
      "pred_index=  1662\n",
      "pred_index=  1663\n",
      "pred_index=  1664\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1665\n",
      "pred_index=  1666\n",
      "pred_index=  1667\n",
      "pred_index=  1668\n",
      "pred_index=  1669\n",
      "pred_index=  1670\n",
      "pred_index=  1671\n",
      "pred_index=  1672\n",
      "pred_index=  1673\n",
      "pred_index=  1674\n",
      "pred_index=  1675\n",
      "pred_index=  1676\n",
      "pred_index=  1677\n",
      "pred_index=  1678\n",
      "pred_index=  1679\n",
      "pred_index=  1680\n",
      "pred_index=  1681\n",
      "pred_index=  1682\n",
      "pred_index=  1683\n",
      "pred_index=  1684\n",
      "pred_index=  1685\n",
      "pred_index=  1686\n",
      "pred_index=  1687\n",
      "pred_index=  1688\n",
      "pred_index=  1689\n",
      "pred_index=  1690\n",
      "pred_index=  1691\n",
      "pred_index=  1692\n",
      "pred_index=  1693\n",
      "pred_index=  1694\n",
      "pred_index=  1695\n",
      "pred_index=  1696\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1697\n",
      "pred_index=  1698\n",
      "pred_index=  1699\n",
      "pred_index=  1700\n",
      "pred_index=  1701\n",
      "pred_index=  1702\n",
      "pred_index=  1703\n",
      "pred_index=  1704\n",
      "pred_index=  1705\n",
      "pred_index=  1706\n",
      "pred_index=  1707\n",
      "pred_index=  1708\n",
      "pred_index=  1709\n",
      "pred_index=  1710\n",
      "pred_index=  1711\n",
      "pred_index=  1712\n",
      "pred_index=  1713\n",
      "pred_index=  1714\n",
      "pred_index=  1715\n",
      "pred_index=  1716\n",
      "pred_index=  1717\n",
      "pred_index=  1718\n",
      "pred_index=  1719\n",
      "pred_index=  1720\n",
      "pred_index=  1721\n",
      "pred_index=  1722\n",
      "pred_index=  1723\n",
      "pred_index=  1724\n",
      "pred_index=  1725\n",
      "pred_index=  1726\n",
      "pred_index=  1727\n",
      "pred_index=  1728\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1729\n",
      "pred_index=  1730\n",
      "pred_index=  1731\n",
      "pred_index=  1732\n",
      "pred_index=  1733\n",
      "pred_index=  1734\n",
      "pred_index=  1735\n",
      "pred_index=  1736\n",
      "pred_index=  1737\n",
      "pred_index=  1738\n",
      "pred_index=  1739\n",
      "pred_index=  1740\n",
      "pred_index=  1741\n",
      "pred_index=  1742\n",
      "pred_index=  1743\n",
      "pred_index=  1744\n",
      "pred_index=  1745\n",
      "pred_index=  1746\n",
      "pred_index=  1747\n",
      "pred_index=  1748\n",
      "pred_index=  1749\n",
      "pred_index=  1750\n",
      "pred_index=  1751\n",
      "pred_index=  1752\n",
      "pred_index=  1753\n",
      "pred_index=  1754\n",
      "pred_index=  1755\n",
      "pred_index=  1756\n",
      "pred_index=  1757\n",
      "pred_index=  1758\n",
      "pred_index=  1759\n",
      "pred_index=  1760\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1761\n",
      "pred_index=  1762\n",
      "pred_index=  1763\n",
      "pred_index=  1764\n",
      "pred_index=  1765\n",
      "pred_index=  1766\n",
      "pred_index=  1767\n",
      "pred_index=  1768\n",
      "pred_index=  1769\n",
      "pred_index=  1770\n",
      "pred_index=  1771\n",
      "pred_index=  1772\n",
      "pred_index=  1773\n",
      "pred_index=  1774\n",
      "pred_index=  1775\n",
      "pred_index=  1776\n",
      "pred_index=  1777\n",
      "pred_index=  1778\n",
      "pred_index=  1779\n",
      "pred_index=  1780\n",
      "pred_index=  1781\n",
      "pred_index=  1782\n",
      "pred_index=  1783\n",
      "pred_index=  1784\n",
      "pred_index=  1785\n",
      "pred_index=  1786\n",
      "pred_index=  1787\n",
      "pred_index=  1788\n",
      "pred_index=  1789\n",
      "pred_index=  1790\n",
      "pred_index=  1791\n",
      "pred_index=  1792\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1793\n",
      "pred_index=  1794\n",
      "pred_index=  1795\n",
      "pred_index=  1796\n",
      "pred_index=  1797\n",
      "pred_index=  1798\n",
      "pred_index=  1799\n",
      "pred_index=  1800\n",
      "pred_index=  1801\n",
      "pred_index=  1802\n",
      "pred_index=  1803\n",
      "pred_index=  1804\n",
      "pred_index=  1805\n",
      "pred_index=  1806\n",
      "pred_index=  1807\n",
      "pred_index=  1808\n",
      "pred_index=  1809\n",
      "pred_index=  1810\n",
      "pred_index=  1811\n",
      "pred_index=  1812\n",
      "pred_index=  1813\n",
      "pred_index=  1814\n",
      "pred_index=  1815\n",
      "pred_index=  1816\n",
      "pred_index=  1817\n",
      "pred_index=  1818\n",
      "pred_index=  1819\n",
      "pred_index=  1820\n",
      "pred_index=  1821\n",
      "pred_index=  1822\n",
      "pred_index=  1823\n",
      "pred_index=  1824\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1825\n",
      "pred_index=  1826\n",
      "pred_index=  1827\n",
      "pred_index=  1828\n",
      "pred_index=  1829\n",
      "pred_index=  1830\n",
      "pred_index=  1831\n",
      "pred_index=  1832\n",
      "pred_index=  1833\n",
      "pred_index=  1834\n",
      "pred_index=  1835\n",
      "pred_index=  1836\n",
      "pred_index=  1837\n",
      "pred_index=  1838\n",
      "pred_index=  1839\n",
      "pred_index=  1840\n",
      "pred_index=  1841\n",
      "pred_index=  1842\n",
      "pred_index=  1843\n",
      "pred_index=  1844\n",
      "pred_index=  1845\n",
      "pred_index=  1846\n",
      "pred_index=  1847\n",
      "pred_index=  1848\n",
      "pred_index=  1849\n",
      "pred_index=  1850\n",
      "pred_index=  1851\n",
      "pred_index=  1852\n",
      "pred_index=  1853\n",
      "pred_index=  1854\n",
      "pred_index=  1855\n",
      "pred_index=  1856\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1857\n",
      "pred_index=  1858\n",
      "pred_index=  1859\n",
      "pred_index=  1860\n",
      "pred_index=  1861\n",
      "pred_index=  1862\n",
      "pred_index=  1863\n",
      "pred_index=  1864\n",
      "pred_index=  1865\n",
      "pred_index=  1866\n",
      "pred_index=  1867\n",
      "pred_index=  1868\n",
      "pred_index=  1869\n",
      "pred_index=  1870\n",
      "pred_index=  1871\n",
      "pred_index=  1872\n",
      "pred_index=  1873\n",
      "pred_index=  1874\n",
      "pred_index=  1875\n",
      "pred_index=  1876\n",
      "pred_index=  1877\n",
      "pred_index=  1878\n",
      "pred_index=  1879\n",
      "pred_index=  1880\n",
      "pred_index=  1881\n",
      "pred_index=  1882\n",
      "pred_index=  1883\n",
      "pred_index=  1884\n",
      "pred_index=  1885\n",
      "pred_index=  1886\n",
      "pred_index=  1887\n",
      "pred_index=  1888\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1889\n",
      "pred_index=  1890\n",
      "pred_index=  1891\n",
      "pred_index=  1892\n",
      "pred_index=  1893\n",
      "pred_index=  1894\n",
      "pred_index=  1895\n",
      "pred_index=  1896\n",
      "pred_index=  1897\n",
      "pred_index=  1898\n",
      "pred_index=  1899\n",
      "pred_index=  1900\n",
      "pred_index=  1901\n",
      "pred_index=  1902\n",
      "pred_index=  1903\n",
      "pred_index=  1904\n",
      "pred_index=  1905\n",
      "pred_index=  1906\n",
      "pred_index=  1907\n",
      "pred_index=  1908\n",
      "pred_index=  1909\n",
      "pred_index=  1910\n",
      "pred_index=  1911\n",
      "pred_index=  1912\n",
      "pred_index=  1913\n",
      "pred_index=  1914\n",
      "pred_index=  1915\n",
      "pred_index=  1916\n",
      "pred_index=  1917\n",
      "pred_index=  1918\n",
      "pred_index=  1919\n",
      "pred_index=  1920\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1921\n",
      "pred_index=  1922\n",
      "pred_index=  1923\n",
      "pred_index=  1924\n",
      "pred_index=  1925\n",
      "pred_index=  1926\n",
      "pred_index=  1927\n",
      "pred_index=  1928\n",
      "pred_index=  1929\n",
      "pred_index=  1930\n",
      "pred_index=  1931\n",
      "pred_index=  1932\n",
      "pred_index=  1933\n",
      "pred_index=  1934\n",
      "pred_index=  1935\n",
      "pred_index=  1936\n",
      "pred_index=  1937\n",
      "pred_index=  1938\n",
      "pred_index=  1939\n",
      "pred_index=  1940\n",
      "pred_index=  1941\n",
      "pred_index=  1942\n",
      "pred_index=  1943\n",
      "pred_index=  1944\n",
      "pred_index=  1945\n",
      "pred_index=  1946\n",
      "pred_index=  1947\n",
      "pred_index=  1948\n",
      "pred_index=  1949\n",
      "pred_index=  1950\n",
      "pred_index=  1951\n",
      "pred_index=  1952\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  1953\n",
      "pred_index=  1954\n",
      "pred_index=  1955\n",
      "pred_index=  1956\n",
      "pred_index=  1957\n",
      "pred_index=  1958\n",
      "pred_index=  1959\n",
      "pred_index=  1960\n",
      "pred_index=  1961\n",
      "pred_index=  1962\n",
      "pred_index=  1963\n",
      "pred_index=  1964\n",
      "pred_index=  1965\n",
      "pred_index=  1966\n",
      "pred_index=  1967\n",
      "pred_index=  1968\n",
      "pred_index=  1969\n",
      "pred_index=  1970\n",
      "pred_index=  1971\n",
      "pred_index=  1972\n",
      "pred_index=  1973\n",
      "pred_index=  1974\n",
      "pred_index=  1975\n",
      "pred_index=  1976\n",
      "pred_index=  1977\n",
      "pred_index=  1978\n",
      "pred_index=  1979\n",
      "pred_index=  1980\n",
      "pred_index=  1981\n",
      "pred_index=  1982\n",
      "pred_index=  1983\n",
      "pred_index=  1984\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  1985\n",
      "pred_index=  1986\n",
      "pred_index=  1987\n",
      "pred_index=  1988\n",
      "pred_index=  1989\n",
      "pred_index=  1990\n",
      "pred_index=  1991\n",
      "pred_index=  1992\n",
      "pred_index=  1993\n",
      "pred_index=  1994\n",
      "pred_index=  1995\n",
      "pred_index=  1996\n",
      "pred_index=  1997\n",
      "pred_index=  1998\n",
      "pred_index=  1999\n",
      "pred_index=  2000\n",
      "pred_index=  2001\n",
      "pred_index=  2002\n",
      "pred_index=  2003\n",
      "pred_index=  2004\n",
      "pred_index=  2005\n",
      "pred_index=  2006\n",
      "pred_index=  2007\n",
      "pred_index=  2008\n",
      "pred_index=  2009\n",
      "pred_index=  2010\n",
      "pred_index=  2011\n",
      "pred_index=  2012\n",
      "pred_index=  2013\n",
      "pred_index=  2014\n",
      "pred_index=  2015\n",
      "pred_index=  2016\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  2017\n",
      "pred_index=  2018\n",
      "pred_index=  2019\n",
      "pred_index=  2020\n",
      "pred_index=  2021\n",
      "pred_index=  2022\n",
      "pred_index=  2023\n",
      "pred_index=  2024\n",
      "pred_index=  2025\n",
      "pred_index=  2026\n",
      "pred_index=  2027\n",
      "pred_index=  2028\n",
      "pred_index=  2029\n",
      "pred_index=  2030\n",
      "pred_index=  2031\n",
      "pred_index=  2032\n",
      "pred_index=  2033\n",
      "pred_index=  2034\n",
      "pred_index=  2035\n",
      "pred_index=  2036\n",
      "pred_index=  2037\n",
      "pred_index=  2038\n",
      "pred_index=  2039\n",
      "pred_index=  2040\n",
      "pred_index=  2041\n",
      "pred_index=  2042\n",
      "pred_index=  2043\n",
      "pred_index=  2044\n",
      "pred_index=  2045\n",
      "pred_index=  2046\n",
      "pred_index=  2047\n",
      "pred_index=  2048\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  2049\n",
      "pred_index=  2050\n",
      "pred_index=  2051\n",
      "pred_index=  2052\n",
      "pred_index=  2053\n",
      "pred_index=  2054\n",
      "pred_index=  2055\n",
      "pred_index=  2056\n",
      "pred_index=  2057\n",
      "pred_index=  2058\n",
      "pred_index=  2059\n",
      "pred_index=  2060\n",
      "pred_index=  2061\n",
      "pred_index=  2062\n",
      "pred_index=  2063\n",
      "pred_index=  2064\n",
      "pred_index=  2065\n",
      "pred_index=  2066\n",
      "pred_index=  2067\n",
      "pred_index=  2068\n",
      "pred_index=  2069\n",
      "pred_index=  2070\n",
      "pred_index=  2071\n",
      "pred_index=  2072\n",
      "pred_index=  2073\n",
      "pred_index=  2074\n",
      "pred_index=  2075\n",
      "pred_index=  2076\n",
      "pred_index=  2077\n",
      "pred_index=  2078\n",
      "pred_index=  2079\n",
      "pred_index=  2080\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  2081\n",
      "pred_index=  2082\n",
      "pred_index=  2083\n",
      "pred_index=  2084\n",
      "pred_index=  2085\n",
      "pred_index=  2086\n",
      "pred_index=  2087\n",
      "pred_index=  2088\n",
      "pred_index=  2089\n",
      "pred_index=  2090\n",
      "pred_index=  2091\n",
      "pred_index=  2092\n",
      "pred_index=  2093\n",
      "pred_index=  2094\n",
      "pred_index=  2095\n",
      "pred_index=  2096\n",
      "pred_index=  2097\n",
      "pred_index=  2098\n",
      "pred_index=  2099\n",
      "pred_index=  2100\n",
      "pred_index=  2101\n",
      "pred_index=  2102\n",
      "pred_index=  2103\n",
      "pred_index=  2104\n",
      "pred_index=  2105\n",
      "pred_index=  2106\n",
      "pred_index=  2107\n",
      "pred_index=  2108\n",
      "pred_index=  2109\n",
      "pred_index=  2110\n",
      "pred_index=  2111\n",
      "pred_index=  2112\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  2113\n",
      "pred_index=  2114\n",
      "pred_index=  2115\n",
      "pred_index=  2116\n",
      "pred_index=  2117\n",
      "pred_index=  2118\n",
      "pred_index=  2119\n",
      "pred_index=  2120\n",
      "pred_index=  2121\n",
      "pred_index=  2122\n",
      "pred_index=  2123\n",
      "pred_index=  2124\n",
      "pred_index=  2125\n",
      "pred_index=  2126\n",
      "pred_index=  2127\n",
      "pred_index=  2128\n",
      "pred_index=  2129\n",
      "pred_index=  2130\n",
      "pred_index=  2131\n",
      "pred_index=  2132\n",
      "pred_index=  2133\n",
      "pred_index=  2134\n",
      "pred_index=  2135\n",
      "pred_index=  2136\n",
      "pred_index=  2137\n",
      "pred_index=  2138\n",
      "pred_index=  2139\n",
      "pred_index=  2140\n",
      "pred_index=  2141\n",
      "pred_index=  2142\n",
      "pred_index=  2143\n",
      "pred_index=  2144\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  2145\n",
      "pred_index=  2146\n",
      "pred_index=  2147\n",
      "pred_index=  2148\n",
      "pred_index=  2149\n",
      "pred_index=  2150\n",
      "pred_index=  2151\n",
      "pred_index=  2152\n",
      "pred_index=  2153\n",
      "pred_index=  2154\n",
      "pred_index=  2155\n",
      "pred_index=  2156\n",
      "pred_index=  2157\n",
      "pred_index=  2158\n",
      "pred_index=  2159\n",
      "pred_index=  2160\n",
      "pred_index=  2161\n",
      "pred_index=  2162\n",
      "pred_index=  2163\n",
      "pred_index=  2164\n",
      "pred_index=  2165\n",
      "pred_index=  2166\n",
      "pred_index=  2167\n",
      "pred_index=  2168\n",
      "pred_index=  2169\n",
      "pred_index=  2170\n",
      "pred_index=  2171\n",
      "pred_index=  2172\n",
      "pred_index=  2173\n",
      "pred_index=  2174\n",
      "pred_index=  2175\n",
      "pred_index=  2176\n",
      "4/4 [==============================] - 2s 494ms/step\n",
      "pred_index=  2177\n",
      "pred_index=  2178\n",
      "pred_index=  2179\n",
      "pred_index=  2180\n",
      "pred_index=  2181\n",
      "pred_index=  2182\n",
      "pred_index=  2183\n",
      "pred_index=  2184\n",
      "pred_index=  2185\n",
      "pred_index=  2186\n",
      "pred_index=  2187\n",
      "pred_index=  2188\n",
      "pred_index=  2189\n",
      "pred_index=  2190\n",
      "pred_index=  2191\n",
      "pred_index=  2192\n",
      "pred_index=  2193\n",
      "pred_index=  2194\n",
      "pred_index=  2195\n",
      "pred_index=  2196\n",
      "pred_index=  2197\n",
      "pred_index=  2198\n",
      "pred_index=  2199\n",
      "pred_index=  2200\n",
      "pred_index=  2201\n",
      "pred_index=  2202\n",
      "pred_index=  2203\n",
      "pred_index=  2204\n",
      "pred_index=  2205\n",
      "pred_index=  2206\n",
      "pred_index=  2207\n",
      "pred_index=  2208\n",
      "4/4 [==============================] - 2s 493ms/step\n",
      "pred_index=  2209\n",
      "pred_index=  2210\n",
      "pred_index=  2211\n",
      "pred_index=  2212\n",
      "pred_index=  2213\n",
      "pred_index=  2214\n",
      "pred_index=  2215\n",
      "pred_index=  2216\n",
      "pred_index=  2217\n",
      "pred_index=  2218\n",
      "pred_index=  2219\n",
      "pred_index=  2220\n",
      "pred_index=  2221\n",
      "pred_index=  2222\n",
      "pred_index=  2223\n",
      "pred_index=  2224\n",
      "pred_index=  2225\n",
      "pred_index=  2226\n",
      "pred_index=  2227\n",
      "pred_index=  2228\n",
      "pred_index=  2229\n",
      "pred_index=  2230\n",
      "pred_index=  2231\n",
      "pred_index=  2232\n",
      "pred_index=  2233\n",
      "pred_index=  2234\n",
      "pred_index=  2235\n",
      "pred_index=  2236\n",
      "pred_index=  2237\n",
      "pred_index=  2238\n",
      "pred_index=  2239\n",
      "pred_index=  2240\n",
      "3/3 [==============================] - 11s 5s/step\n",
      "pred_index=  2241\n",
      "pred_index=  2242\n",
      "pred_index=  2243\n",
      "pred_index=  2244\n",
      "pred_index=  2245\n",
      "pred_index=  2246\n",
      "pred_index=  2247\n",
      "pred_index=  2248\n",
      "pred_index=  2249\n",
      "pred_index=  2250\n",
      "pred_index=  2251\n",
      "pred_index=  2252\n",
      "pred_index=  2253\n",
      "pred_index=  2254\n",
      "pred_index=  2255\n",
      "pred_index=  2256\n",
      "pred_index=  2257\n",
      "pred_index=  2258\n",
      "pred_index=  2259\n",
      "pred_index=  2260\n",
      "pred_index=  2261\n",
      "pred_index=  2262\n"
     ]
    }
   ],
   "source": [
    "ins_min = []\n",
    "\n",
    "for pred_index in range(num_preds):\n",
    "\n",
    "    print(\"pred_index= \", pred_index)\n",
    "    permuted_preds_matrix = preds_stream[pred_index]\n",
    "    map = ut_dense(permuted_preds_matrix, diagonal_offset=2)\n",
    "    # scd = calculate_SCD(permuted_map, ref_map)\n",
    "    # scd_mean = np.mean(scd)\n",
    "    \n",
    "    average_map = map.mean(axis=2)\n",
    "    ins = calculate_min_insulation(average_map)\n",
    "    ins_min.append(ins)\n",
    "                            \n",
    "    # all_data.append(permuted_map)\n",
    "    \n",
    "    # if (pred_index % 100 == 0) and (pred_index != 0):\n",
    "    #     all_data = np.array(all_data)\n",
    "    #     np.save(f\"/scratch1/smaruj/test_human_fold0_AkitaV2/pred_matrices_{pred_index}.npy\", all_data)\n",
    "    #     all_data = []\n",
    "        \n",
    "    # SCD_list.append(scd_mean)\n",
    "    # print(scd)\n",
    "    # print(\"mean: \", scd_mean)\n",
    "    # print(scd)\n",
    "    # plot_map(permuted_map[:,:,1])\n",
    "    # print(\"difference map\")\n",
    "    # plot_map(permuted_map[:,:,1]-ref_map[:,:,1])\n",
    "    \n",
    "    # write_stat_metrics_to_h5(\n",
    "    #         permuted_preds_matrix,\n",
    "    #         ref_preds_matrix,\n",
    "    #         stats_out,\n",
    "    #         exp_index,\n",
    "    #         head_index,\n",
    "    #         model_index,\n",
    "    #         diagonal_offset=2,\n",
    "    #         stat_metrics=stats,\n",
    "    #     )\n",
    "    # exp_index += 1\n",
    "\n",
    "# all_data = np.array(all_data)   \n",
    "# np.save(f\"/scratch1/smaruj/test_mouse_fold0_AkitaV2/pred_matrices_800.npy\", all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexplained_tads[\"ins16_min\"] = ins_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCD_h1_m0_t0</th>\n",
       "      <th>SCD_h1_m0_t1</th>\n",
       "      <th>SCD_h1_m0_t2</th>\n",
       "      <th>SCD_h1_m0_t3</th>\n",
       "      <th>SCD_h1_m0_t4</th>\n",
       "      <th>SCD_h1_m0_t5</th>\n",
       "      <th>chrom</th>\n",
       "      <th>end</th>\n",
       "      <th>rel_disruption_end</th>\n",
       "      <th>rel_disruption_start</th>\n",
       "      <th>start</th>\n",
       "      <th>type</th>\n",
       "      <th>window_end</th>\n",
       "      <th>window_start</th>\n",
       "      <th>miss_per</th>\n",
       "      <th>ins16_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.625</td>\n",
       "      <td>13.9800</td>\n",
       "      <td>9.8400</td>\n",
       "      <td>9.760</td>\n",
       "      <td>10.5600</td>\n",
       "      <td>8.8000</td>\n",
       "      <td>chr1</td>\n",
       "      <td>4410000</td>\n",
       "      <td>667648</td>\n",
       "      <td>665600</td>\n",
       "      <td>4400000</td>\n",
       "      <td>up4</td>\n",
       "      <td>5061504</td>\n",
       "      <td>3750784</td>\n",
       "      <td>9.879583</td>\n",
       "      <td>-0.204339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.717</td>\n",
       "      <td>0.8345</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>chr1</td>\n",
       "      <td>4410000</td>\n",
       "      <td>620544</td>\n",
       "      <td>618496</td>\n",
       "      <td>4400000</td>\n",
       "      <td>down15</td>\n",
       "      <td>5061504</td>\n",
       "      <td>3750784</td>\n",
       "      <td>9.879583</td>\n",
       "      <td>-0.204339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.633</td>\n",
       "      <td>1.6190</td>\n",
       "      <td>1.6640</td>\n",
       "      <td>1.480</td>\n",
       "      <td>1.4375</td>\n",
       "      <td>1.1750</td>\n",
       "      <td>chr1</td>\n",
       "      <td>5160000</td>\n",
       "      <td>622592</td>\n",
       "      <td>620544</td>\n",
       "      <td>5150000</td>\n",
       "      <td>down14</td>\n",
       "      <td>5811504</td>\n",
       "      <td>4500784</td>\n",
       "      <td>14.626723</td>\n",
       "      <td>-0.233104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.366</td>\n",
       "      <td>1.4660</td>\n",
       "      <td>1.3620</td>\n",
       "      <td>1.321</td>\n",
       "      <td>1.3210</td>\n",
       "      <td>1.1160</td>\n",
       "      <td>chr18</td>\n",
       "      <td>73180000</td>\n",
       "      <td>620544</td>\n",
       "      <td>618496</td>\n",
       "      <td>73170000</td>\n",
       "      <td>down15</td>\n",
       "      <td>73831504</td>\n",
       "      <td>72520784</td>\n",
       "      <td>16.061542</td>\n",
       "      <td>-0.285407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.480</td>\n",
       "      <td>14.4100</td>\n",
       "      <td>14.8400</td>\n",
       "      <td>15.160</td>\n",
       "      <td>13.9500</td>\n",
       "      <td>11.8360</td>\n",
       "      <td>chr1</td>\n",
       "      <td>6200000</td>\n",
       "      <td>651264</td>\n",
       "      <td>649216</td>\n",
       "      <td>6190000</td>\n",
       "      <td>tad1</td>\n",
       "      <td>6851504</td>\n",
       "      <td>5540784</td>\n",
       "      <td>17.484202</td>\n",
       "      <td>-0.343384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>1.087</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>1.361</td>\n",
       "      <td>1.1460</td>\n",
       "      <td>1.0230</td>\n",
       "      <td>chr16</td>\n",
       "      <td>21450000</td>\n",
       "      <td>618496</td>\n",
       "      <td>616448</td>\n",
       "      <td>21440000</td>\n",
       "      <td>down16</td>\n",
       "      <td>22101504</td>\n",
       "      <td>20790784</td>\n",
       "      <td>7.270233</td>\n",
       "      <td>-0.294114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>9.890</td>\n",
       "      <td>9.0900</td>\n",
       "      <td>10.0400</td>\n",
       "      <td>10.080</td>\n",
       "      <td>9.6600</td>\n",
       "      <td>8.3100</td>\n",
       "      <td>chr9</td>\n",
       "      <td>121710000</td>\n",
       "      <td>647168</td>\n",
       "      <td>645120</td>\n",
       "      <td>121700000</td>\n",
       "      <td>down2</td>\n",
       "      <td>122361504</td>\n",
       "      <td>121050784</td>\n",
       "      <td>9.879583</td>\n",
       "      <td>-0.271511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>8.640</td>\n",
       "      <td>8.5500</td>\n",
       "      <td>8.8800</td>\n",
       "      <td>8.586</td>\n",
       "      <td>8.4700</td>\n",
       "      <td>7.2800</td>\n",
       "      <td>chr9</td>\n",
       "      <td>122360000</td>\n",
       "      <td>651264</td>\n",
       "      <td>649216</td>\n",
       "      <td>122350000</td>\n",
       "      <td>tad1</td>\n",
       "      <td>123011504</td>\n",
       "      <td>121700784</td>\n",
       "      <td>10.986476</td>\n",
       "      <td>-0.177083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>1.611</td>\n",
       "      <td>1.3910</td>\n",
       "      <td>2.0600</td>\n",
       "      <td>2.105</td>\n",
       "      <td>1.7330</td>\n",
       "      <td>1.5070</td>\n",
       "      <td>chr7</td>\n",
       "      <td>122190000</td>\n",
       "      <td>618496</td>\n",
       "      <td>616448</td>\n",
       "      <td>122180000</td>\n",
       "      <td>down16</td>\n",
       "      <td>122841504</td>\n",
       "      <td>121530784</td>\n",
       "      <td>2.325502</td>\n",
       "      <td>-0.129382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>14.914</td>\n",
       "      <td>16.0500</td>\n",
       "      <td>21.8900</td>\n",
       "      <td>21.380</td>\n",
       "      <td>19.8300</td>\n",
       "      <td>17.2800</td>\n",
       "      <td>chr9</td>\n",
       "      <td>123140000</td>\n",
       "      <td>677888</td>\n",
       "      <td>675840</td>\n",
       "      <td>123130000</td>\n",
       "      <td>up9</td>\n",
       "      <td>123791504</td>\n",
       "      <td>122480784</td>\n",
       "      <td>6.517865</td>\n",
       "      <td>-0.207170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2263 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SCD_h1_m0_t0  SCD_h1_m0_t1  SCD_h1_m0_t2  SCD_h1_m0_t3  SCD_h1_m0_t4  \\\n",
       "0           12.625       13.9800        9.8400         9.760       10.5600   \n",
       "1            0.717        0.8345        0.9307         0.885        0.8260   \n",
       "2            1.633        1.6190        1.6640         1.480        1.4375   \n",
       "3            1.366        1.4660        1.3620         1.321        1.3210   \n",
       "4           12.480       14.4100       14.8400        15.160       13.9500   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2258         1.087        0.9897        1.3480         1.361        1.1460   \n",
       "2259         9.890        9.0900       10.0400        10.080        9.6600   \n",
       "2260         8.640        8.5500        8.8800         8.586        8.4700   \n",
       "2261         1.611        1.3910        2.0600         2.105        1.7330   \n",
       "2262        14.914       16.0500       21.8900        21.380       19.8300   \n",
       "\n",
       "      SCD_h1_m0_t5  chrom        end  rel_disruption_end  \\\n",
       "0           8.8000   chr1    4410000              667648   \n",
       "1           0.6606   chr1    4410000              620544   \n",
       "2           1.1750   chr1    5160000              622592   \n",
       "3           1.1160  chr18   73180000              620544   \n",
       "4          11.8360   chr1    6200000              651264   \n",
       "...            ...    ...        ...                 ...   \n",
       "2258        1.0230  chr16   21450000              618496   \n",
       "2259        8.3100   chr9  121710000              647168   \n",
       "2260        7.2800   chr9  122360000              651264   \n",
       "2261        1.5070   chr7  122190000              618496   \n",
       "2262       17.2800   chr9  123140000              677888   \n",
       "\n",
       "      rel_disruption_start      start    type  window_end  window_start  \\\n",
       "0                   665600    4400000     up4     5061504       3750784   \n",
       "1                   618496    4400000  down15     5061504       3750784   \n",
       "2                   620544    5150000  down14     5811504       4500784   \n",
       "3                   618496   73170000  down15    73831504      72520784   \n",
       "4                   649216    6190000    tad1     6851504       5540784   \n",
       "...                    ...        ...     ...         ...           ...   \n",
       "2258                616448   21440000  down16    22101504      20790784   \n",
       "2259                645120  121700000   down2   122361504     121050784   \n",
       "2260                649216  122350000    tad1   123011504     121700784   \n",
       "2261                616448  122180000  down16   122841504     121530784   \n",
       "2262                675840  123130000     up9   123791504     122480784   \n",
       "\n",
       "       miss_per  ins16_min  \n",
       "0      9.879583  -0.204339  \n",
       "1      9.879583  -0.204339  \n",
       "2     14.626723  -0.233104  \n",
       "3     16.061542  -0.285407  \n",
       "4     17.484202  -0.343384  \n",
       "...         ...        ...  \n",
       "2258   7.270233  -0.294114  \n",
       "2259   9.879583  -0.271511  \n",
       "2260  10.986476  -0.177083  \n",
       "2261   2.325502  -0.129382  \n",
       "2262   6.517865  -0.207170  \n",
       "\n",
       "[2263 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unexplained_tads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexplained_tads.to_csv(\"all_unexplained_boundaries.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"/scratch1/smaruj/test_mouse_fold0_AkitaV2/combined_pred_matrices.npy\", all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_open.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basenji_py3.9_tf2.15",
   "language": "python",
   "name": "basenji_py3.9_tf2.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
